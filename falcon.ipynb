{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoModel\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " თავდაპირველაღ ღმერთმა შექმნა  <->  tavdap’irvelagh ghmertma shek\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from utils import latkar, karlat, get_bible\n",
    "\n",
    "ge_bible = get_bible()\n",
    "en_bible = karlat(ge_bible)\n",
    "\n",
    "print(ge_bible[:30], \"<->\", en_bible[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' თავდაპირველაღ ღმერთმა შექმნა ცა და მიწა. ',\n",
       " ' მიწა იყო უსახო და უდაბური, ბნელი იდო უფსკრულზე და სული ღვთისა იძვროდა წყლებს ზემოთ. ',\n",
       " ' თქვა ღმერთმა: იყოს ნათელი! და იქმნა ნათელი. ',\n",
       " ' და ნახა ღმერთმა, რომ ნათელი კარგი იყო, და გაჰყარა ღმერთმა ნათელი და ბნელი. ',\n",
       " ' ნათელს ღმერთმა უწოდა დღე და ბნელს უწოდა ღამე. იყო საღამო, იყო დილა პირველი დღე. ',\n",
       " ' თქვა ღმერთმა: იყოს წყალთა შორის მყარი და გაჰყაროს წყლები. ',\n",
       " ' გააჩინა ღმერთმა მყარი და გაჰყარა ერთმანეთისგან წყალი, რომელიც არის მყარს ქვემოთ, და წყალი, რომელიც არის მყარს ზემოთ. და იქმნა ასე. ',\n",
       " ' მყარს ღმერთმა უწოდა ცა. იყო საღამო, იყო დილა - მეორე დღე.  ',\n",
       " ' თქვა ღმერთმა: შეგროვდეს ერთგან ცისქვეშეთის წყალი და გამოჩნდეს ხმელეთი. და იქმნა ასე. ',\n",
       " ' ხმელეთს ღმერთმა უწოდა მიწა და შეგროვილ წყალს უწოდა ზღვა. დაინახა ღმერთმა, რომ კარგი იყო. 11 . თქვა ღმერთმა: აღმოაცენოს მიწამ მცენარეული - ბალახი, თესლის მთესველი, ხე ნაყოფიერი, თესლოვანი ნაყოფის მომტანი მიწაზე თავისი გვარისდა მიხედვით. და იქმნა ასე. ',\n",
       " ' წარმოშვა მიწამ მცენარეული - ბალახი, თესლის მთესველი თავისი გვარისდა მიხედვით, და ხე, თესლოვანი ნაყოფის მომტანი, თავისი გვარისდა მიხედვით. ',\n",
       " ' იყო საღამო, იყო დილა - მესამე დღე. ',\n",
       " ' თქვა ღმერთმა: იყოს მნათობები ცის მყარზე დღისა და ღამის გასაყრელად, დროჟამის აღმნიშვნელად - დღეებისა და წელიწადებისა; ',\n",
       " ' იყვნენ მანათობლებად ცის მყარზე რომ გაანათონ მიწა. და იქმნა ასე. ',\n",
       " ' გააჩინა ღმერთმა ორი მთავარი მნათობი, - დიდი მნათობი დღის განმგებლად და მცირე მნათობი ღამის განმგებლად - და ვარსკვლავები. ',\n",
       " ' დასხა ისინი ღმერთმა ცის მყარზე, რომ გაენათებინათ მიწა, ',\n",
       " ' განეგოთ დღე და ღამე, გაეყარათ ნათელი და ბნელი. დაინახა ღმერთმა, რომ კარგი იყო. ',\n",
       " ' იყო საღამო, იყო დილა - მეოთხე დღე. ',\n",
       " ' თქვა ღმერთმა: აფუთფუთდეს წყალში სულდგმული; მიწის ზემოთ კი, ცის მყარზე, ფრინველმა იფრინოს. და იქმნა ასე. 21 . შექმნა ღმერთმა დიდი თევზები და ყოველი სულდგმული, მცურავი თავ-თავისი გვარისდა მიხედვით, რაც-კი წყალში ფუთფუთებს, და ყველა ფრთოსანი თავ-თავისი გვარისდა მიხედვით. დაინახა ღმერთმა, რომ კარგი იყო. ',\n",
       " ' აკურთხა ღმერთმა ისინი და თქვა: ინაყოფიერეთ და იმრავლეთ, აავსეთ ზღვები. ფრინველებმა იმრავლონ მიწაზე. ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Split on numbering (e.g 1., 2., 3., )\n",
    "ls = re.split(r\"\\d+\\.\", ge_bible)\n",
    "ls[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 33642\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3738\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' utkhra samuelma mtels israels: aha, q’velaperi mogisminet, rasats moitkhovdit chemgan da dagisvit mepe. '}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ls = [karlat(s) for s in ls]\n",
    "dataset = Dataset.from_dict({\"text\": ls})\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "print(dataset)\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/33642 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 54893\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6058\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        chunks += [sentence[i:i + 100] for i in range(0, len(sentence), 100)]\n",
    "    return {\"text\": chunks}\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    r =  examples\n",
    "    # r = \n",
    "    # assert isinstance(r, str), r\n",
    "    # r = karlat(r)\n",
    "    out = tokenizer(r['text'], truncation=True, max_length=100)\n",
    "    # for ex in examples[\"text\"]:\n",
    "    #     r = tokenizer(ex)\n",
    "    return out\n",
    "\n",
    "dataset = dataset.map(\n",
    "    chunk_examples,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================WARNING: DEPRECATED!==============================\n",
      "WARNING! This version of bitsandbytes is deprecated. Please switch to `pip install bitsandbytes` and the new repo: https://github.com/TimDettmers/bitsandbytes\n",
      "==============================WARNING: DEPRECATED!==============================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nmodule 'bitsandbytes.nn' has no attribute 'Linear8bitLt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/utils/import_utils.py:1084\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1084\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1085\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:212\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m is_peft_available():\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel\n\u001b[1;32m    215\u001b[0m skip_first_batches \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.3.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING, get_peft_config, get_peft_model\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     PeftModel,\n\u001b[1;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/peft/mapping.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     PeftModel,\n\u001b[1;32m     18\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     19\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     20\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     21\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     25\u001b[0m     AdaptionPromptConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     PromptTuningConfig,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/peft/peft_model.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     AdaLoraModel,\n\u001b[1;32m     33\u001b[0m     AdaptionPromptModel,\n\u001b[1;32m     34\u001b[0m     LoraModel,\n\u001b[1;32m     35\u001b[0m     PrefixEncoder,\n\u001b[1;32m     36\u001b[0m     PromptEmbedding,\n\u001b[1;32m     37\u001b[0m     PromptEncoder,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001b[1;32m     41\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     shift_tokens_right,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/peft/tuners/__init__.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madaption_prompt\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlora\u001b[39;00m \u001b[39mimport\u001b[39;00m LoraConfig, LoraModel\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madalora\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/peft/tuners/lora.py:666\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m--> 666\u001b[0m     \u001b[39mclass\u001b[39;00m \u001b[39mLinear8bitLt\u001b[39;00m(bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mLinear8bitLt, LoraLayer):\n\u001b[1;32m    667\u001b[0m         \u001b[39m# Lora implemented in a dense layer\u001b[39;00m\n\u001b[1;32m    668\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    669\u001b[0m             \u001b[39mself\u001b[39m,\n\u001b[1;32m    670\u001b[0m             adapter_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    677\u001b[0m         ):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'bitsandbytes.nn' has no attribute 'Linear8bitLt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, TrainingArguments, Trainer\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DataCollatorForLanguageModeling\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtiiuae/falcon-7b\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      6\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16,\n\u001b[1;32m      7\u001b[0m     trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/utils/import_utils.py:1074\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1073\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1074\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1075\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1076\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/utils/import_utils.py:1086\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1085\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1087\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nmodule 'bitsandbytes.nn' has no attribute 'Linear8bitLt'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '__func__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m\n\u001b[1;32m      4\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39m# push_to_hub=True,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     17\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/transformers/trainer.py:1756\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1754\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mprepare(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m   1755\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1756\u001b[0m         model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mprepare(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n\u001b[1;32m   1757\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1758\u001b[0m     \u001b[39m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m     model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mprepare(\n\u001b[1;32m   1760\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\n\u001b[1;32m   1761\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1182\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_megatron_lm(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\n\u001b[1;32m   1183\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_one(obj, first_pass\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_placement\u001b[39m=\u001b[39;49md) \u001b[39mfor\u001b[39;49;00m obj, d \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(args, device_placement)\n\u001b[1;32m   1184\u001b[0m     )\n\u001b[1;32m   1185\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_one(obj, device_placement\u001b[39m=\u001b[39md) \u001b[39mfor\u001b[39;00m obj, d \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1187\u001b[0m \u001b[39mif\u001b[39;00m tpu_should_fix_optimizer \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_precision \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfp8\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1188\u001b[0m     \u001b[39m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1183\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1180\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_megatron_lm(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[0;32m-> 1183\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_one(obj, first_pass\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_placement\u001b[39m=\u001b[39;49md) \u001b[39mfor\u001b[39;00m obj, d \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, device_placement)\n\u001b[1;32m   1184\u001b[0m     )\n\u001b[1;32m   1185\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_one(obj, device_placement\u001b[39m=\u001b[39md) \u001b[39mfor\u001b[39;00m obj, d \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(result, device_placement))\n\u001b[1;32m   1187\u001b[0m \u001b[39mif\u001b[39;00m tpu_should_fix_optimizer \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_precision \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfp8\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1188\u001b[0m     \u001b[39m# 2. grabbing new model parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1022\u001b[0m, in \u001b[0;36mAccelerator._prepare_one\u001b[0;34m(self, obj, first_pass, device_placement)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_data_loader(obj, device_placement\u001b[39m=\u001b[39mdevice_placement)\n\u001b[1;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m-> 1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_model(obj, device_placement\u001b[39m=\u001b[39;49mdevice_placement)\n\u001b[1;32m   1023\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mOptimizer):\n\u001b[1;32m   1024\u001b[0m     optimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_optimizer(obj, device_placement\u001b[39m=\u001b[39mdevice_placement)\n",
      "File \u001b[0;32m/opt/conda/envs/torch/lib/python3.10/site-packages/accelerate/accelerator.py:1311\u001b[0m, in \u001b[0;36mAccelerator.prepare_model\u001b[0;34m(self, model, device_placement, evaluation_mode)\u001b[0m\n\u001b[1;32m   1308\u001b[0m     model\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m MethodType(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16)(model\u001b[39m.\u001b[39mforward\u001b[39m.\u001b[39m\u001b[39m__func__\u001b[39m), model)\n\u001b[1;32m   1309\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixed_precision \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbf16\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mTPU:\n\u001b[1;32m   1310\u001b[0m     model\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m MethodType(\n\u001b[0;32m-> 1311\u001b[0m         torch\u001b[39m.\u001b[39mautocast(device_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16)(model\u001b[39m.\u001b[39;49mforward\u001b[39m.\u001b[39;49m\u001b[39m__func__\u001b[39;49m), model\n\u001b[1;32m   1312\u001b[0m     )\n\u001b[1;32m   1313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1314\u001b[0m     model\u001b[39m.\u001b[39mforward \u001b[39m=\u001b[39m MethodType(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast()(model\u001b[39m.\u001b[39mforward\u001b[39m.\u001b[39m\u001b[39m__func__\u001b[39m), model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute '__func__'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    # bf16=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en = tokenizer.encode(\n",
    "    ge_bible[:38]\n",
    ")\n",
    "dec = tokenizer.decode(en)\n",
    "print(\"GE bible\", len(en), dec, en)\n",
    "\n",
    "\n",
    "en = tokenizer.encode(\n",
    "    en_bible[:35]\n",
    ")\n",
    "dec = tokenizer.decode(en)\n",
    "print(\"EN bible\", len(en), dec, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(en_bible[:35], return_tensors='pt')\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# Keep top 30 logits at max; stop if cumulative probability >= 1.0.\n",
    "top_logits = top_k_top_p_filtering(last_layer_logits, top_k=100, top_p=1.0)\n",
    "\n",
    "# Softmax the logits into probabilities\n",
    "probabilities = F.softmax(top_logits, dim=-1)\n",
    "\n",
    "# Generate next token\n",
    "generated_next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "generated = torch.cat([inputs, generated_next_token], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get result\n",
    "result_string = tokenizer.decode(generated.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ge_bible[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latkar(en_bible[:38]), '->', latkar(result_string), 'actual', latkar(ge_bible[:40]))\n",
    "print(en_bible[:38], '->', result_string, 'actual', ge_bible[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     trust_remote_code=True,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# sequences = pipeline(\n",
    "#    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "#     # max_length=200,\n",
    "#     # do_sample=True,\n",
    "#     # top_k=10,\n",
    "#     # num_return_sequences=1,\n",
    "#     # eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
