{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "# text_source = \"https://github.com/karpathy/char-rnn/raw/master/data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " თავდაპირველაღ ღმერთმა შექმნა ცა და მიწა.  მიწა იყო უსახო და უდაბური, ბნელი იდო უფსკრულზე და სული ღვთისა იძვროდა წყლებს ზემოთ.  თქვა ღმერთმა: იყოს ნათელი! და იქმნა ნათელი.  და ნახა ღმერთმა, რომ ნათელი კარგი იყო, და გაჰყარა ღმერთმა ნათელი და ბნელი.  ნათელს ღმერთმა უწოდა დღე და ბნელს უწოდა ღამე. იყო ს 3789044 ო თუ ვინმე რამეს მოაკლებს ამ წიგნის წინასწარმეტყველების სიტყვებს, ღმერთი მოაკლებს მას წილს სიცოცხლის ხიდან, წმიდა ქალაქიდან და იქიდან, რაც ჩაწერილია ამ წიგნში.  ამის დამმოწმებელი ამბობს: ჰო, მოვალ მალე! ამინ; ჰო, მოდი, უფალო იესო!  ჩვენი უფლის, იესო ქრისტეს მადლი ყველა თქვენგანთან. ამინ.  დასასრული.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "bible = open('data/bible.txt', 'r').read()\n",
    "bible = BeautifulSoup(bible, \"lxml\").text\n",
    "# Remove digits\n",
    "bible = bible[165:-55]\n",
    "bible = re.sub(r'\\d+.', '', bible)\n",
    "# Remove brackets, \n",
    "bible = re.sub( r'\\([^)]*\\)', '', bible)\n",
    "# Remove symbol ჲ\n",
    "bible = re.sub(r'ჲ', '', bible)\n",
    "bible = re.sub(r'\\*', '', bible)\n",
    "bible = re.sub(r'\\[', '', bible)\n",
    "bible = re.sub(r'\\]', '', bible)\n",
    "# Replace all different types of quotes with \"\n",
    "bible = re.sub(r'[\\u201c\\u201d\\u201e\\u201f\\u2033\\u2036]', '\"', bible)\n",
    "print(bible[:300], len(bible), bible[-300:])\n",
    "text = bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789044"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " თავდაპირველაღ ღმერთმა შექმნა ცა და მიწა.  მიწა იყო უსახო და უდაბური, ბნელი იდო უფსკრულზე და სული ღვთისა იძვროდა წყლებს ზემოთ.  თქვა ღმერთმა: იყოს ნათელი! და იქმნა ნათელი.  და ნახა ღმერთმა, რომ ნათელი კარგი იყო, და გაჰყარა ღმერთმა ნათელი და ბნელი.  ნათელს ღმერთმა უწოდა დღე და ბნელს უწოდა ღამე. იყო საღამო, იყო დილა პირველი დღე.  თქვა ღმერთმა: იყოს წყალთა შორის მყარი და გაჰყაროს წყლები.  გააჩინა ღმერთმა მყარი და გაჰყარა ერთმანეთისგან წყალი, რომელიც არის მყარს ქვემოთ, და წყალი, რომელიც არის მყარს ზემოთ. და იქმნა ასე.  მყარს ღმერთმა უწოდა ცა. იყო საღამო, იყო დილა - მეორე დღე.   თქვა ღმერთმა: შეგროვდეს ერთგან ცისქვეშეთის წყალი და გამოჩნდეს ხმელეთი. და იქმნა ასე.  ხმელეთს ღმერთმა უწოდა მიწა და შეგროვილ წყალს უწოდა ზღვა. დაინახა ღმერთმა, რომ კარგი იყო. . თქვა ღმერთმა: აღმოაცენოს მიწამ მცენარეული - ბალახი, თესლის მთესველი, ხე ნაყოფიერი, თესლოვანი ნაყოფის მომტანი მიწაზე თავისი გვარისდა მიხედვით. და იქმნა ასე.  წარმოშვა მიწამ მცენარეული - ბალახი, თესლის მთესველი თავისი გვარისდა მიხედვით, და ხე, \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' !\",-.:;?აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ',\n",
       " 42,\n",
       " 42,\n",
       " Counter({' ': 550556,\n",
       "          'ა': 486303,\n",
       "          'ი': 343975,\n",
       "          'ე': 279051,\n",
       "          'ს': 206772,\n",
       "          'მ': 188980,\n",
       "          'რ': 160018,\n",
       "          'დ': 154199,\n",
       "          'ნ': 134090,\n",
       "          'ო': 132565,\n",
       "          'ვ': 130373,\n",
       "          'ლ': 126604,\n",
       "          'თ': 110472,\n",
       "          'ბ': 87970,\n",
       "          'უ': 74252,\n",
       "          'გ': 67688,\n",
       "          ',': 66562,\n",
       "          'ხ': 59504,\n",
       "          'ც': 50926,\n",
       "          'შ': 48635,\n",
       "          '.': 34613,\n",
       "          'ყ': 32804,\n",
       "          'კ': 32372,\n",
       "          'ქ': 32285,\n",
       "          'წ': 28404,\n",
       "          'ფ': 24203,\n",
       "          'ღ': 23314,\n",
       "          'ტ': 22158,\n",
       "          'ზ': 17345,\n",
       "          'ჩ': 15726,\n",
       "          'ძ': 15225,\n",
       "          'პ': 10259,\n",
       "          ':': 8904,\n",
       "          ';': 5649,\n",
       "          'ჭ': 5357,\n",
       "          'ჰ': 5085,\n",
       "          '-': 4694,\n",
       "          'ჯ': 4614,\n",
       "          '?': 3145,\n",
       "          '!': 1907,\n",
       "          'ჟ': 1150,\n",
       "          '\"': 336}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "''.join(chars), len(chars), vocab_size, Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([17, 32, 22], 'ცად')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "encode(\"იყო\"), decode(encode(\"ცად\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3789044]) tensor(0) tensor(41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 'cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.min(), data.max()), device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 256\n",
    "\n",
    "# train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) -> tensor(16)\n",
      "tensor([ 0, 16]) -> tensor(9)\n",
      "tensor([ 0, 16,  9]) -> tensor(14)\n",
      "tensor([ 0, 16,  9, 14]) -> tensor(12)\n",
      "tensor([ 0, 16,  9, 14, 12]) -> tensor(9)\n",
      "tensor([ 0, 16,  9, 14, 12,  9]) -> tensor(23)\n",
      "tensor([ 0, 16,  9, 14, 12,  9, 23]) -> tensor(17)\n",
      "tensor([ 0, 16,  9, 14, 12,  9, 23, 17]) -> tensor(25)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:8]\n",
    "y = train_data[1:8+1]\n",
    "for t in range(8):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(context, \"->\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix, ix.shape, ix.min(), ix.max(), ix.dtype)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = torch.zeros((1,1), device=device, dtype=torch.long)\n",
    "# print(decode(m.generate(context, 100).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "@torch.no_grad()\n",
    "def estimate_loss(m: nn.Module):\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            xb, yb = get_batch('valid')\n",
    "            _, loss = m(xb, yb)\n",
    "            losses[i] = loss\n",
    "        out[split] = f\"Mean: {losses.mean().item():.3f}, Var: {losses.var().item():.3f}\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "# B,T,C = 4,8,32 # batch, time, channel\n",
    "# x = torch.randn(B,T,C)\n",
    "\n",
    "# # Single head of SA\n",
    "# # Emit q, and v\n",
    "# head_size = 16\n",
    "# key = nn.Linear(C, head_size, bias=False)\n",
    "# query = nn.Linear(C, head_size, bias=False)\n",
    "# value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# # Each token emits 16 values (q, k) pairs (head size)\n",
    "\n",
    "# k = key(x)\n",
    "# print(\"k = \", k.var().item())\n",
    "\n",
    "# k = k / (C ** .5) # B,T,HeadSize\n",
    "# q = query(x) # B,T,HeadSize\n",
    "# v = value(x)\n",
    "\n",
    "# print(\"K=\",k.var().item(), '1/hs^2 =',head_size ** -.5)\n",
    "\n",
    "# # wei = torch.zeros((T,T)) # this is made from q dot v \n",
    "# wei = q @ k.permute(0,2,1) # (B,T,H) @ (B,H,T) -> (B,T,T)\n",
    "# print(q.var(), k.var(), wei.var())\n",
    "\n",
    "# tril = torch.tril(torch.ones(T,T))\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# out = wei @ v\n",
    "# # out.shape, out\n",
    "# out.shape, x.var(), out.var(), k.var(), q.var(), v.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single head torch.Size([4, 8, 32]) torch.Size([4, 8, 16]) 16\n",
      "Multi head torch.Size([4, 8, 32]) torch.Size([4, 8, 32]) 16\n"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "# batch_size = 32 * 4\n",
    "n_embed = 32 \n",
    "head_size = 16\n",
    "n_heads = 4\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\n",
    "            'tril', torch.tril(torch.ones(block_size, block_size))\n",
    "        )\n",
    "    def forward(self, x: Tensor):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,H\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei: Tensor = q @ k.permute(0,2,1) * C ** -0.5 # B,T,T\n",
    "        wei.masked_fill_(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        return wei @ v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, head_size: int):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.n_heads = n_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        # self.linear = nn.Linear(n_heads * head_size, n_embed, bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = torch.concat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "h = Head(head_size=head_size)\n",
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "out = h(x)\n",
    "print(\"Single head\", x.shape, out.shape, head_size)\n",
    "\n",
    "\n",
    "\n",
    "h = MultiHeadAttention(n_heads=4, head_size=n_embed//4)\n",
    "x = torch.randn(batch_size, block_size, n_embed)\n",
    "out = h(x)\n",
    "print(\"Multi head\", x.shape, out.shape, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True) # INCORRECT\n",
    "        return (x - mean) / (std + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embed * 4, n_embed),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.2040, grad_fn=<VarBackward0>), torch.Size([8, 32]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_heads=n_head, head_size=head_size)\n",
    "        self.ffwd = FeedForward(n_embed=n_embed)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(x)\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "    \n",
    "b = Block(n_embed=n_embed, n_head=n_heads)\n",
    "x = torch.randn(32, block_size, n_embed)\n",
    "x = b(x)[0]\n",
    "x.var(), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP: 0 Loss {'train': 'Mean: 3.981, Var: 0.020', 'valid': 'Mean: 3.966, Var: 0.019'}\n",
      " შჟქჭკს აჭვიჭჩბდტჭხჭფქ!ჭ!ქფ!დჭ.ჭჯზზჯძქოეტკფ ;-იტფტჩცჩ-კქაჭექ!ძყლნ!ოთხხზ!ძ\"ზზეჩჭჩჩ?შჩ;ქ!ჭხღჟსჯჭ;ჯჟგთო\".ჭჰფჭუსჩქჟქ!ჩზღ!ბვშ-\n",
      "STEP: 500 Loss {'train': 'Mean: 2.618, Var: 0.052', 'valid': 'Mean: 2.617, Var: 0.039'}\n",
      " ღმა წაძელითნელთაგა, ჟავყიადა მათწლიყუვის ჩვისაენოთგრა ძაავერს ჭძას მოცდა ვემო კიდავანის ველოწი: სალიულძოქმება ოთდათელთდა\n",
      "STEP: 1000 Loss {'train': 'Mean: 2.491, Var: 0.046', 'valid': 'Mean: 2.505, Var: 0.039'}\n",
      " ჩიხალრემდედ თარისა სი გათელინიწას მიგელი მთხვითის, მოვაპუუდ, შ უფერგი ჭ ძელეთა ტაღვათლტეტენ ჩეზემი .იდრე  იშამინს შიროდ \n",
      "STEP: 1500 Loss {'train': 'Mean: 2.471, Var: 0.054', 'valid': 'Mean: 2.465, Var: 0.049'}\n",
      " თიოს როხა მიცადებია ფაატინ: ქვეუეტელ მი, როხლწებთლა მეცს დამითულოძმებიდან ხოლიხა, თქვიდარმურმისა: ბომმეთიმ ხვერც წარიაც \n",
      "STEP: 2000 Loss {'train': 'Mean: 2.413, Var: 0.043', 'valid': 'Mean: 2.416, Var: 0.040'}\n",
      " ხაც ისა სახუი ჩომოვილიდიას დამივისგევრც ყველხატმცა მემესნქვარვანი მო! სოკარარი, ტყრენუ იღარით, რიზეს ბან თხოხლისები მერუ\n",
      "STEP: 2500 Loss {'train': 'Mean: 2.398, Var: 0.055', 'valid': 'Mean: 2.382, Var: 0.051'}\n",
      " ნიბამ ათ ერმას ყტოჰ.  მია ხებოთ ჩეუწიმე თქეცადებებიააზან  ვერირნა მისეფთ ჭაჰ მის შეგჭენ ერაცხა პ იამ დაა ბა დანანე  აჩენ\n",
      "STEP: 3000 Loss {'train': 'Mean: 2.381, Var: 0.057', 'valid': 'Mean: 2.395, Var: 0.063'}\n",
      " რაბვით ძაყოთოვინე: ფოსხოლისულეს ა- წაენეს თავიდდა ესცისი უდგოამოზემე შგიმაკან, შემა: როსა? ჭერ, და რომელსცა ვა და ა ღმალ\n",
      "STEP: 3500 Loss {'train': 'Mean: 2.342, Var: 0.053', 'valid': 'Mean: 2.380, Var: 0.050'}\n",
      " იყვინა დამო გავმანენაენთდარასებთ და დღეიდენ.  თაგავილის.  წვემის თუ მიძსა: თქართ ად გაბუდირსოს ქაბთარჩენენენნ, ვენუს მიძ\n",
      "STEP: 4000 Loss {'train': 'Mean: 2.375, Var: 0.052', 'valid': 'Mean: 2.330, Var: 0.060'}\n",
      " მაგვართაებ ღვიდლებნი რომლენი ებმეს: და იმაგოუთხა აჰყვლის აც გამართხონიეას ძისფალისჭელტას იალმდები!თ მისებურის ქარითი და \n",
      "STEP: 4500 Loss {'train': 'Mean: 2.382, Var: 0.059', 'valid': 'Mean: 2.354, Var: 0.053'}\n",
      " მათა მემ ფომე ტეობნთი ცონი ქვოსთან უპო მიოსინი, უფო ხშიოსგგან ეხისხდად სიგვეობიც -ჩიმიწი.  ემიეყოდან მიქი თვვენ.  პონობო\n",
      "STEP: 5000 Loss {'train': 'Mean: 2.300, Var: 0.061', 'valid': 'Mean: 2.338, Var: 0.068'}\n",
      " თავ: სულისნა!  ახველს ჰავიაღსნ არ დადათ, შემისა საუწკება, თქვენ ურას! და ჩან ვენ სამ შვილმათ მასომს იოყოვროდა სათვი, აცე\n",
      "STEP: 5500 Loss {'train': 'Mean: 2.299, Var: 0.055', 'valid': 'Mean: 2.351, Var: 0.060'}\n",
      " ჩრორგამი ყვლება ადა დგანაი რძინებთ დასრა მგვით, მა ბზამ ნზევის და ათდილი სასჯოლს შეშის ყოლდა საკებს და ვდა ფალს უფი.  ამ\n",
      "STEP: 6000 Loss {'train': 'Mean: 2.326, Var: 0.085', 'valid': 'Mean: 2.289, Var: 0.060'}\n",
      " სეთი მაჩიან ჩემი არ დავიდრების ღმოპებზე თაცი ლესომასმოთთებს.  სართავანდა იესისი მილი მათილი მევ მინდი, ზკითის ხალიტების \n",
      "STEP: 6500 Loss {'train': 'Mean: 2.297, Var: 0.054', 'valid': 'Mean: 2.304, Var: 0.061'}\n",
      " ქიალად უღანები, როცებზე შიმიწყვდებიბმეტლოცეთან შუთქვეს, ისითმეფლის მაშიელს ყიატიველიც და ანთვიცლოლების კითინი ხა დირძულა\n",
      "STEP: 7000 Loss {'train': 'Mean: 2.312, Var: 0.072', 'valid': 'Mean: 2.276, Var: 0.053'}\n",
      " ბრთი ჯარის უ;  აღროსგავს: ჩამო ისრომ მა, არ თქვეტის; დაიმაელია, ვენრე და გააკადებია კაცენუ მსრად. ერთხდა, ეზაშენ, თავილა\n",
      "STEP: 7500 Loss {'train': 'Mean: 2.303, Var: 0.068', 'valid': 'Mean: 2.324, Var: 0.076'}\n",
      " ფალის  შელი ჟინათუ-ა.  დაგნეთა კეთს ახრა, თაოავიელის რადგან მეთვრებე ექმაბულით თველით სიყვე ქვეყყლებთა კვდილოს ისტანებს \n",
      "STEP: 8000 Loss {'train': 'Mean: 2.276, Var: 0.065', 'valid': 'Mean: 2.247, Var: 0.062'}\n",
      " წზრაებას, იოდონცად გაუძღრთ ირჩეს მოას, ზებს ძმაცეს, რომდებყლადაშმა, პირომყი, რომ დღეს უნამდეცას კი, იმოვდათ; კრან მოზე ს\n",
      "STEP: 8500 Loss {'train': 'Mean: 2.271, Var: 0.068', 'valid': 'Mean: 2.247, Var: 0.054'}\n",
      " უფალის სწესვაპოლმა, და კითილით.  არ ასგრმმარიმ და იხიდან, გობაცაყუედა მოერძრებოფა, და ღყული.  მაში, რა გალოყორილა.  ციგე\n",
      "STEP: 9000 Loss {'train': 'Mean: 2.226, Var: 0.065', 'valid': 'Mean: 2.229, Var: 0.058'}\n",
      "  თავენი გაიკროდლეს გამოშაავნი ასაქმდა უცხლი უსიტყვლიბან და საწყრები , მარაუმ მოგობ ენეთან გავაცში ჩააქი. სიტკიმი უფლა, მ\n",
      "STEP: 9500 Loss {'train': 'Mean: 2.239, Var: 0.064', 'valid': 'Mean: 2.240, Var: 0.068'}\n",
      " აღტებელთაკვანი, მიართა, იმ კვთი გარგაუკე ჩემს ქვეყველი დადგანან.  განადის, თავალთად ჩემი მწანირეთ თავისხვი პურდელი ძოვაკ\n"
     ]
    }
   ],
   "source": [
    "def print_tensor(name, t: torch.Tensor):\n",
    "    return\n",
    "    print(name, f\"{tuple(t.shape)}, m: {t.mean().item():.4f}, v: {t.var().item():.4f}\")\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 \n",
    "                 vocab_size: int,\n",
    "                 n_embed: int) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, n_embed\n",
    "        )\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            block_size, n_embed\n",
    "        )\n",
    "        # # self.sa_head = Head(n_embed)\n",
    "        # self.multi_sa = MultiHeadAttention(\n",
    "        #     n_heads=4,\n",
    "        #     head_size=n_embed // 4,\n",
    "        # ) # 4 heads, each using 8 dim qkv vectors, and later combine into 32 output vec\n",
    "        # self.ffwd = FeedForward(n_embed=n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "            Block(n_embed, n_head=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            n_embed, vocab_size\n",
    "        )\n",
    "    def forward(self, idx: torch.Tensor, targets = None): \n",
    "        B,T = idx.shape\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(T, device=device)\n",
    "        ) # [T,C]\n",
    "        # print_tensor(\"pos_emb\", pos_emb)\n",
    "        tok_emb = self.embedding(idx) # [B,T,C]\n",
    "        # print_tensor(\"tok_emb\", tok_emb)\n",
    "        x = pos_emb + tok_emb # [B,T, n_embed]\n",
    "        # print_tensor(\"x\", x)\n",
    "        # x = self.sa_head(x)\n",
    "        # x = self.multi_sa(x) # [B,T, n_embed]\n",
    "\n",
    "        # x = self.ffwd(x)\n",
    "        # for b in self.blocks:\n",
    "        x = self.blocks(x)\n",
    "            \n",
    "        # print_tensor(\"x (sa)\", x)\n",
    "        logits = self.lm_head(x) # [B,T,VocabSize]\n",
    "        # print_tensor(\"logits\", logits)\n",
    "        B,T,C = logits.size()\n",
    "        # wei = torch.tril(torch.ones(T,T))\n",
    "        # wei = torch.masked_fill(wei == 0, float('-inf'))\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None \n",
    "        else:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B*T,C), targets.view(B*T),\n",
    "            )\n",
    "            return logits, loss\n",
    "        return logits, loss\n",
    "        # return logits\n",
    "    def generate(self, \n",
    "                idx, # [B, T]\n",
    "                max_new_tokens: int):\n",
    "        idx = idx.to(device)\n",
    "        for i in range(max_new_tokens):\n",
    "            # print(idx.shape, idx[:,-block_size:].shape, block_size)\n",
    "            logits, loss = self(idx[:,-block_size:]) \n",
    "            logits: torch.Tensor = logits # [B,T,C]\n",
    "            logits = logits[:,-1,:] # [B,C]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(\n",
    "                probs, num_samples=1,\n",
    "            )\n",
    "            idx = torch.cat([idx, next_tokens], dim=1)\n",
    "        # print(idx, idx.shape)\n",
    "        return idx.cpu()\n",
    "\n",
    "m = BigramLanguageModel(\n",
    "    vocab_size=vocab_size, n_embed=n_embed\n",
    ")\n",
    "m.to(device)\n",
    "opt = torch.optim.AdamW(\n",
    "    m.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "for step in range(1000 * 10):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss  = m(xb, yb)\n",
    "    loss: torch.Tensor = loss\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if step % 500 == 0:\n",
    "        print(\"STEP:\", step, \"Loss\", estimate_loss(m))\n",
    "        s = decode(m.generate(torch.zeros((1,1)).long(), 120)[0].tolist())\n",
    "        print(s.replace('\\n', ' '))\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.position_embedding_table(\n",
    "    torch.arange(1)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C) # B,T,C\n",
    "wei = torch.tril(torch.ones(T,T)) # T,T\n",
    "wei = wei / wei.sum(1, keepdim=True) # (T,T) / (T,1) -> T,T [Row normed to 1s]\n",
    "xbow2 = wei @ x # (T,T) @ (B,T,C) -> B,T,C\n",
    "# torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 7.],\n",
       "         [2., 0.],\n",
       "         [5., 3.],\n",
       "         [5., 0.],\n",
       "         [4., 0.],\n",
       "         [2., 0.],\n",
       "         [7., 6.],\n",
       "         [0., 8.]]),\n",
       " tensor([[5.0000, 7.0000],\n",
       "         [3.5000, 3.5000],\n",
       "         [4.0000, 3.3333],\n",
       "         [4.2500, 2.5000],\n",
       "         [4.2000, 2.0000],\n",
       "         [3.8333, 1.6667],\n",
       "         [4.2857, 2.2857],\n",
       "         [3.7500, 3.0000]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "B,T,C = 4, 8, 2\n",
    "# x = torch.arange(B*T*C).view(B,T,C).float()\n",
    "# y = torch.zeros(B,T,C)\n",
    "# for b in range(B):\n",
    "#     for t in range(T):\n",
    "#         xprev = x[b,:t+1]\n",
    "#         # print(\"Average of\", xprev.tolist(),\n",
    "#         #        \"on\", x[b].tolist(), \"To get to \", x[b,t].tolist())\n",
    "#         y[b,t] = xprev.mean(dim=0)\n",
    "\n",
    "# print(x[0])\n",
    "# print(y[0])\n",
    "\n",
    "# B,T,C = 4, 8, 2\n",
    "\n",
    "# x = torch.randn(B,T,C) # B,T,C\n",
    "x = torch.randint(0,10,(B,T,C)).float()\n",
    "# xbow = x.cumsum(dim=1) / torch.ones_like(x).cumsum(1)\n",
    "# xbow = x.div(T).cumsum(dim=1)\n",
    "\n",
    "w = torch.ones(8, 8).tril() \n",
    "print(w)\n",
    "w =  (1/torch.arange(1, 9)).diag() @ w\n",
    "print(w)\n",
    "# w = w.unsqueeze(0).repeat(B,1,1)\n",
    "# xbow, w @ x[0], x[0]\n",
    "x[0], w @ x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# B,T,C = 64, 1000, 256\n",
    "# x = torch.randn(B,T,C, device='cuda') # B,T,C\n",
    "# # xbow = x.cumsum(dim=1) / torch.ones_like(x).cumsum(1)\n",
    "# xbox = x.div(T).cumsum(dim=1)\n",
    "# xbox.sum()\n",
    "# assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 7.],\n",
       "         [6., 4.],\n",
       "         [6., 5.]]),\n",
       " tensor([[ 2.,  7.],\n",
       "         [ 8., 11.],\n",
       "         [14., 16.]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)).float()\n",
    "# a = torch.tensor(\n",
    "#     [[1.,1.],\n",
    "#      [1.,1.]]\n",
    "# )\n",
    "# a = torch.tensor(\n",
    "#     [[1,0,1.],\n",
    "#      [0,1,0.],\n",
    "#      [1,0,1.],]\n",
    "# )\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "# c = a @ b\n",
    "# c = a @ b.T\n",
    "# a,b.T,c\n",
    "\n",
    "a,b,a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  7.],\n",
       "        [ 8., 11.],\n",
       "        [14., 16.]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.cumsum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 7.],\n",
       "         [6., 4.],\n",
       "         [6., 5.]]),\n",
       " tensor([[14., 16.],\n",
       "         [14., 16.],\n",
       "         [14., 16.]]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3],\n",
      "         [ 4,  5],\n",
      "         [ 6,  7],\n",
      "         [ 8,  9],\n",
      "         [10, 11],\n",
      "         [12, 13],\n",
      "         [14, 15]],\n",
      "\n",
      "        [[16, 17],\n",
      "         [18, 19],\n",
      "         [20, 21],\n",
      "         [22, 23],\n",
      "         [24, 25],\n",
      "         [26, 27],\n",
      "         [28, 29],\n",
      "         [30, 31]],\n",
      "\n",
      "        [[32, 33],\n",
      "         [34, 35],\n",
      "         [36, 37],\n",
      "         [38, 39],\n",
      "         [40, 41],\n",
      "         [42, 43],\n",
      "         [44, 45],\n",
      "         [46, 47]],\n",
      "\n",
      "        [[48, 49],\n",
      "         [50, 51],\n",
      "         [52, 53],\n",
      "         [54, 55],\n",
      "         [56, 57],\n",
      "         [58, 59],\n",
      "         [60, 61],\n",
      "         [62, 63]]]) torch.Size([4, 8, 2])\n",
      "tensor([[[ 0.,  1.],\n",
      "         [ 1.,  2.],\n",
      "         [ 2.,  3.],\n",
      "         [ 3.,  4.],\n",
      "         [ 4.,  5.],\n",
      "         [ 5.,  6.],\n",
      "         [ 6.,  7.],\n",
      "         [ 7.,  8.]],\n",
      "\n",
      "        [[16., 17.],\n",
      "         [17., 18.],\n",
      "         [18., 19.],\n",
      "         [19., 20.],\n",
      "         [20., 21.],\n",
      "         [21., 22.],\n",
      "         [22., 23.],\n",
      "         [23., 24.]],\n",
      "\n",
      "        [[32., 33.],\n",
      "         [33., 34.],\n",
      "         [34., 35.],\n",
      "         [35., 36.],\n",
      "         [36., 37.],\n",
      "         [37., 38.],\n",
      "         [38., 39.],\n",
      "         [39., 40.]],\n",
      "\n",
      "        [[48., 49.],\n",
      "         [49., 50.],\n",
      "         [50., 51.],\n",
      "         [51., 52.],\n",
      "         [52., 53.],\n",
      "         [53., 54.],\n",
      "         [54., 55.],\n",
      "         [55., 56.]]]) torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4, 8, 2\n",
    "x = torch.arange(B * T * C).view(B,T,C) # B,T,C\n",
    "print(x, x.shape)\n",
    "y = x.cumsum(dim=1) / torch.ones_like(x).cumsum(1)\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss tensor(1., grad_fn=<SumBackward0>) Weight 2.0\n",
      "Grad 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linreg(x,W,b):\n",
    "    return x * W + b\n",
    "\n",
    "x = torch.tensor(1., )\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(0., requires_grad=True)\n",
    "out = linreg(\n",
    "    x,\n",
    "    w,\n",
    "    b,\n",
    ")\n",
    "y = torch.tensor(1.)\n",
    "loss = ((out - y)).abs().sum()\n",
    "print(\"Loss\", loss, \"Weight\", w.item())\n",
    "# loss.backward()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Grad\", w.grad.item())\n",
    "w = w - w.grad\n",
    "\n",
    "linreg(\n",
    "    x,w,b\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
